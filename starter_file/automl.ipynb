{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "TODO: In this markdown cell, give an overview of the dataset you are using. Also mention the task you will be performing.\n",
    "\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'capstone-automl-experiment'\n",
    "\n",
    "experiment=Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or Attach an AmlCompute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: update the cluster name to match the existing cluster\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"automl-com-clst\" #automl-com-clst #automl-com-clst\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "# NOTE: update the key to match the dataset name\n",
    "found = False\n",
    "#key = \"BankMarketing Dataset\"\n",
    "key=\"Maternal_Health_Risk_Data_Set\" #\"bankmarketing_train\" #bankmarketing_train\n",
    "#bankmarketing_train\n",
    "description_text = \"UCI machine Learning Maternal Health Risk Data Set\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "#https://archive.ics.uci.edu/ml/machine-learning-databases/00639/Maternal%20Health%20Risk%20Data%20Set.csv\n",
    "#https://archive.ics.uci.edu/ml/machine-learning-databases/00638/Acoustic%20Features.csv\n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        example_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00639/Maternal%20Health%20Risk%20Data%20Set.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(example_data)        \n",
    "        #Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Dataset Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.take(5).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_d, TEST_d = train_test_split(df, shuffle=True)\n",
    "TRAIN_d, TEST_d = train_test_split(concat_x_y, test_size=0.2, random_state=24, shuffle=True)\n",
    "TRAIN_d.to_csv('train.csv',index = False)\n",
    "\n",
    "ds_new = ws.get_default_datastore()\n",
    "ds_new.upload_files(files = ['./train.csv'])\n",
    "#ds_new.upload(src_dir='.',target_path='.')\n",
    "\n",
    "\n",
    "train_data = Dataset.Tabular.from_delimited_files(path=[(ds_new,('train.csv'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "#### This creates a general AutoML settings object. Udacity notes: These inputs must match what was used when training in the portal. label_column_name has to be \"RiskLevel\" for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    " # \"primary_metric\" : 'AUC_weighted'\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"n_cross_validations\":3,\n",
    "    \"primary_metric\" : 'accuracy'\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=train_data,\n",
    "                             label_column_name=\"RiskLevel\",   \n",
    "                             #path = project_folder,\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             enable_onnx_compatible_models=True\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "#remote_run = experiment.submit(automl_config)\n",
    "remote_run = experiment.submit(config= automl_config, show_output= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(best_model_output._path_on_datastore, \"rb\" ) as f:\n",
    "#    best_model = pickle.load(f)\n",
    "#best_model\n",
    "automl_best_run, automl_fitted_model = remote_run.get_output()\n",
    "print(\"===== AutoMl Best Run ========\")\n",
    "print(automl_best_run)\n",
    "print(\"\\n  ===== AutoMl Best Fitted Model ========\")\n",
    "print(automl_fitted_model)\n",
    "automl_best_run.get_tags()\n",
    "automl_best_run_metrics = automl_best_run.get_metrics()\n",
    "\n",
    "for metric_name in automl_best_run_metrics:\n",
    "    metric = automl_best_run_metrics[metric_name]\n",
    "    print(\"\\n\",metric_name, metric)\n",
    "    \n",
    "\n",
    "#automl_best_run, automl_fitted_model = remote_run.get_output()\n",
    "#print(hasattr(automl_fitted_model, 'steps'))\n",
    "#print(\"\\n\")\n",
    "#print(automl_best_run.get_metrics())\n",
    "#print(\"\\n\")\n",
    "#print(automl_best_run.get_tags())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(automl_fitted_model.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model using Onnx\n",
    "#from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
    "\n",
    "#automl_best_run, automl_fitted_model = remote_run.get_output(return_onnx_model=True)\n",
    "\n",
    "#OnnxConverter.save_onnx_model(automl_fitted_model, './outputs/Automl_Onnex_Model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = best_run.register_model(model_name='best_autoML_run', model_path='outputs/Automl_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python\n",
    "####  Workflow for deploying a model \n",
    "The workflow is similar no matter where you deploy your model:\n",
    "\n",
    "<li>Register the model.\n",
    "<li>Prepare an entry script.\n",
    "<li>Prepare an inference configuration.\n",
    "<li>Deploy the model locally to ensure everything works.\n",
    "<li>Choose a compute target.\n",
    "<li>Deploy the model to the cloud.\n",
    "<li>Test the resulting web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "automl_best_run.register_model(model_path='outputs/Maternal_Health_Risk_model.pkl', \n",
    "                               model_name='automl_Maternal_Health_Risk_model_pickled',\n",
    "                        tags={'Training context':'Maternal Health Risk Auto ML Model Trained'},\n",
    "                        properties={'AUC': automl_best_run_metrics['AUC_weighted'], \n",
    "                                    'Accuracy': automl_best_run_metrics['accuracy']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  List the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare an inference configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "automl_best_run.download_file(constants.CONDA_ENV_FILE_PATH, 'project_environment.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare an inference configuration using an entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "#from azureml.core.environment import Environment\n",
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment(name=\"project_environment\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",environment=env)\n",
    "\n",
    "#inference_config = InferenceConfig(\n",
    "#    environment=env,\n",
    "#    source_directory=\"./source_dir\",\n",
    "#    entry_script=\"./echo_score.py\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model locally to ensure everything works. Will skip this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azureml.core.webservice import LocalWebservice\n",
    "#deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "##Deploy your machine learning model\n",
    "#service = Model.deploy(\n",
    "#    ws,\n",
    "#    \"myservice\",\n",
    "#    [model],\n",
    "#    dummy_inference_config,\n",
    "#    deployment_config,\n",
    "#    overwrite=True,\n",
    "#)\n",
    "#service.wait_for_deployment(show_output=True)\n",
    "#print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "service=Model.deploy(workspace=ws,\n",
    "                    name=\"Maternal-Health-Risk-deploy-service\",\n",
    "                    models=[model],\n",
    "                    inference_config=inference_config,\n",
    "                    deployment_config=deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(service.get_logs())\n",
    "\n",
    "service.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Define a dummy entry script\n",
    "\n",
    "#The entry script receives data submitted to a deployed web service and passes it to the model.\n",
    "#It then returns the model's response to the client. The script is specific to your model. \n",
    "#The entry script must understand the data that the model expects and returns.\n",
    "\n",
    "#The two things you need to accomplish in your entry script are:\n",
    "\n",
    "#Loading your model (using a function called init())\n",
    "#Running your model on input data (using a function called run())\n",
    "#For your initial deployment, use a dummy entry script that prints the data it receives.\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def init():\n",
    "    print(\"This is init\")\n",
    "\n",
    "\n",
    "def run(data):\n",
    "    test = json.loads(data)\n",
    "    print(f\"received data {test}\")\n",
    "    return f\"test is {test}\"\n",
    "\n",
    "#Save this file as echo_score.py inside of a directory called source_dir. This dummy script returns the data you send to it, so it doesn't use the model. But it is useful for testing that the scoring script is running.\n",
    "\n",
    "#The following example demonstrates how to create a minimal environment with no pip dependencies,\n",
    "#using the dummy scoring script you defined above\n",
    "\n",
    "\n",
    "\n",
    "#The following Python demonstrates how to create a local deployment configuration:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
